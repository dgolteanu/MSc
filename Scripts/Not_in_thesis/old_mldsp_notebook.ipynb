{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from scipy import fft\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from scipy.stats import pearsonr\n",
    "# import Bio\n",
    "from Bio.Phylo.TreeConstruction import *\n",
    "from functools import partial\n",
    "import pywt\n",
    "# import os\n",
    "from statistics import median, mean\n",
    "from one_dimensional_num_mapping import *\n",
    "# plotting\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing as pe\n",
    "import h5py\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import io "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLDSP mains scipt imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, plot_confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from collections import defaultdict\n",
    "\n",
    "## Creates and trains various classifiers (linear-discriminant, linear svm, quadratic svm, fine knn) on input sequences and labels\n",
    "## Parameters:\n",
    "# - dismat (array): distance matrix between sequences\n",
    "# - alabels (list): labels corresponding to individual sequences\n",
    "# - folds (int): number of folds to use when splitting dataset\n",
    "# - total (int): total number of sequences\n",
    "# - saveModels (bool): whether or not to save models to file\n",
    "## Return:\n",
    "# - avgAccuracy (float): mean accuracy across all models\n",
    "# - meanModelAccuracies (dictionary): mean accuracy per model\n",
    "# - aggregatedCMatrix (dictionary): aggregated confusion matrix per model\n",
    "# - misclassifiedIdx (dictionary): misclassified indices per model\n",
    "# @profile(sort_by='cumulative', lines_to_print=10, strip_dirs=True)\n",
    "def classify_dismat(dismat, alabels, folds, total, saveModels=False):\n",
    "\n",
    "    kf = KFold(n_splits=folds, shuffle=False, random_state=None)\n",
    "    model_names = {'LinearDiscriminant':LinearDiscriminantAnalysis() #matlab doesn't specify what solver it uses, orginal code used (shrinkage) gamma=0 \n",
    "                   ,'LinearSVM':SVC(kernel='linear',cache_size=1000,decision_function_shape='ovo'), 'QuadSVM':SVC(kernel='poly', degree=2,cache_size=1000,decision_function_shape='ovo'),\n",
    "                   'KNN':KNeighborsClassifier(n_neighbors=1, leaf_size=50, metric='euclidean', weights='uniform', algorithm='brute')\n",
    "                   }\n",
    "    # use 2 additional classifiers if <= 2000 sequences\n",
    "    # if total <= 2000:\n",
    "    #     model_names['SubspaceDiscriminant'] = SVC(kernel='rbf')\n",
    "    #     model_names['SubspaceKNN'] = None\n",
    "\n",
    "    accuracies = defaultdict(list) # dictionary with key: modelname, value: list containing accuracies\n",
    "    confMatrixDict = defaultdict(list) # dictionary with key: modelname, value: list containing confusion matrix displays\n",
    "    misclassifiedIdx = defaultdict(list) # dictionary with key: modelname, value: list containing indices/sequences of dismat that have been misclassifed\n",
    "\n",
    "    # Loop through each model\n",
    "    for modelName in model_names:\n",
    "        model = model_names.get(modelName)\n",
    "        print(model)\n",
    "        # Create pipeline model\n",
    "        if modelName in ['LinearSVM', 'QuadSVM', 'KNN']:\n",
    "            pipeModel = make_pipeline(StandardScaler(), model)\n",
    "        else:\n",
    "            pipeModel = make_pipeline(model)\n",
    "            \n",
    "        i =0\n",
    "        for train_index, test_index in kf.split(dismat, alabels):\n",
    "            i += 1\n",
    "            X_train = dismat[train_index]\n",
    "            X_test = dismat[test_index]\n",
    "            y_train = [alabels[i] for i in train_index]\n",
    "            y_test = [alabels[i] for i in test_index]\n",
    "\n",
    "            # Fit the pipeline model\n",
    "            pipeModel.fit(X_train, y_train)\n",
    "            prediction = pipeModel.predict(X_test)\n",
    "            # Compute and store accuracy of model\n",
    "            accuracies[modelName].append(accuracy_score(y_test, prediction))\n",
    "            print(accuracy_score(y_test, prediction))\n",
    "            # Generate and store confusion matrix\n",
    "            cm = confusion_matrix(y_test, prediction, labels=list(np.unique(alabels)), normalize=None)\n",
    "            confMatrixDict[modelName].append(cm)\n",
    "\n",
    "            # Store indices (of dismat) of misclassified sequences\n",
    "            for i in range(len(prediction)):\n",
    "                # if prediction incorrect, add to list of misclassified indices for the model\n",
    "                if prediction[i] != y_test[i]:\n",
    "                    misclassifiedIdx[modelName].append(test_index[i])\n",
    "            print(i)\n",
    "\n",
    "    # For each model, Calculate mean of accuracies across 10 folds & Sum all confusion matrices across 10 folds\n",
    "    meanModelAccuracies = {} # key: modelName, value: mean accuracy value for model\n",
    "    aggregatedCMatrix = {} # key: modelName, value: summed Confusion Matrix for model\n",
    "    for modelName in accuracies:\n",
    "        meanModelAccuracies[modelName] = np.mean(accuracies.get(modelName))\n",
    "        aggregatedCMatrix[modelName] = np.sum(confMatrixDict.get(modelName), axis=0)\n",
    "\n",
    "    # Mean accuracy value across all classifiers\n",
    "    avgAccuracy = sum(meanModelAccuracies.values()) / len(meanModelAccuracies)\n",
    "\n",
    "    return avgAccuracy, meanModelAccuracies, aggregatedCMatrix, dict(misclassifiedIdx)\n",
    "\n",
    "# Plots and returns a ConfusionMatrix Display object from a raw array\n",
    "def displayConfusionMatrix(confMatrix, alabels):\n",
    "    # generate cm image and plot\n",
    "    confMatrixDisplayObj = ConfusionMatrixDisplay(confusion_matrix=confMatrix, display_labels=list(np.unique(alabels)))\n",
    "    confMatrixDisplayObj.plot(cmap='Blues', colorbar= False)\n",
    "\n",
    "    # access raw cm array: cm_disp.confusion_matrix\n",
    "    # alternative to display: cm_disp = plot_confusion_matrix(pipeModel, X_test, y_test, normalize=None, cmap='Blues', colorbar= False)\n",
    "\n",
    "    return confMatrixDisplayObj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing.py\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "\n",
    "def preprocessing(data_set, max_clust_size, metadata):\n",
    "    \"\"\"Preprocessing of fasta sequences using BioPython into a database of\n",
    "    SeqRecord objects each representing a unique sequence, can handle multiple\n",
    "    sequence fastas.\n",
    "\n",
    "    seqs: main sequence database, dictionary-like object, no info on clusters\n",
    "    cluster_names: list of all cluster names from sub-directory names.\n",
    "\n",
    "    number_of_clusters: integer of the total count of clusters.\n",
    "\n",
    "    cluster_sample_info: Dictionary with keys=cluster_names and values =\n",
    "    a tuple consisting of: (number of samples in cluster,\n",
    "        a list of accession ids corresponding to sequences of that cluster).\n",
    "\n",
    "    total_seq: integer of the total sequence count in dataset.\n",
    "\n",
    "    cluster_dict: depracated with cluster_sample_info, will be removed\n",
    "    \"\"\"\n",
    "    # Dictionary to store SeqIO\n",
    "    seq_dict = {}\n",
    "    # dictionary with Accession ID as keys and cluster name as values\n",
    "    cluster_dict = {}\n",
    "    bad_keys=[]\n",
    "   \n",
    "    # Iterate through all fasta files\n",
    "    for f in sorted(os.listdir(data_set)):\n",
    "        file = os.path.join(data_set, f)\n",
    "        with open(file) as handle:\n",
    "            # SeqIO.index_db() is read only & can't multiprocess, SeqIO.index doesnt take file handle for multi-file, SeqIO_to_dict is all in memory, SeqIO.parse you only get to iterate over data once\n",
    "    #         # single dict\n",
    "            seq_dict.update(SeqIO.to_dict(SeqIO.parse(handle, \"fasta\")))\n",
    "            \n",
    "    cluster_dict = pd.read_csv(metadata,header=None, index_col=0, sep=None).squeeze(\"columns\").to_dict()\n",
    "    cluster_stats = Counter(cluster_dict.values())\n",
    "    \n",
    "    for key in seq_dict.keys():\n",
    "        if not key in cluster_dict:\n",
    "            bad_keys.append(key)\n",
    "    \n",
    "    print(bad_keys)\n",
    "    #Might affect order of labels & lead to mislabelling\n",
    "    # for item in bad_keys:\n",
    "    #     seq_dict.pop(item)\n",
    "    \n",
    "    total_seq = len(seq_dict)\n",
    "    return seq_dict, total_seq, cluster_dict, cluster_stats\n",
    "\n",
    "\n",
    "def old_preprocessing(data_set, max_clust_size):\n",
    "    # Dictionary to store SeqIO\n",
    "    seq_dict = {}\n",
    "    cluster_names = sorted(os.listdir(data_set))\n",
    "    # dictionary with Accession ID as keys and cluster name as values\n",
    "    cluster_dict = {}\n",
    "    # number of samples in each cluster\n",
    "    # cluster_samples_info = {}\n",
    "    # count of the number of clusters as int\n",
    "    cluster_stats={}\n",
    "    # Iterate over each cluster (top level directories)\n",
    "    for cluster in cluster_names:\n",
    "        files = os.listdir(os.path.join(data_set, cluster))\n",
    "        files = [os.path.join(data_set, cluster, f) for f in files]\n",
    "        # paths.extend(files)\n",
    "        # get path for cluster as str\n",
    "        cluster_path = os.path.join(data_set, cluster)\n",
    "        # get names of files in cluster as list of str\n",
    "        file_name = sorted(os.listdir(cluster_path))\n",
    "        cluster_stats.update({cluster:len(file_name)})\n",
    "        temp_dict={}\n",
    "        # Iterate over each file in the cluster\n",
    "        for file in file_name:\n",
    "            # get path for each file in cluster as str\n",
    "            file_path = os.path.join(cluster_path, file)\n",
    "            # Required to use SeqIO.index to generate dictionary of SeqRecords (parsed on demand in main script)\n",
    "\n",
    "            # SeqIO.index doesnt take file handle to index multiple seqs to a\n",
    "            # single dict\n",
    "\n",
    "            # Not sure if storing the dict like object of SeqIO.index in a dict forces loading into memory (performance penalty)\n",
    "            seqs = SeqIO.index(file_path, \"fasta\"\n",
    "            #,key_function=get_accession\n",
    "            )\n",
    "            seq_dict.update(seqs)\n",
    "            # Generate second dictionary for cluster info\n",
    "            for accession_id in seqs.keys():\n",
    "                cluster_dict.update({accession_id: cluster})\n",
    "        # if len(temp_dict) >= max_clust_size:\n",
    "        #     subset = dict(random.sample(temp_dict.items(), max_clust_size))\n",
    "        #     print(len(subset))\n",
    "        #     seq_dict.update(subset)\n",
    "        # else:\n",
    "        #     seq_dict.update(temp_dict)\n",
    "        # for accession_id in seq_dict.keys():\n",
    "        #     cluster_dict.update({accession_id: cluster}) \n",
    "    total_seq = len(seq_dict)\n",
    "    del temp_dict \n",
    "    del seqs\n",
    "    return seq_dict, total_seq, cluster_dict, cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS, TSNE\n",
    "from skbio.stats.ordination import pcoa\n",
    "\n",
    "def dimReduction(data, n_dim, method):\n",
    "    \"\"\"\n",
    "    Function will take in a nxm 2d-array and reduce the dimensions of the data using a specified dimensionality\n",
    "    reduction technique (PCA, MDS, or TSNE).\n",
    "    :param np.array data: input data to be transformed\n",
    "    :param int n_dim: dimensions to reduce to\n",
    "    :param str method: which method to use (either 'pca', 'mds', or 'tsne')\n",
    "    :return np.array transformed: nxn_dim array of tranformed data\n",
    "    \"\"\"\n",
    "    if method == 'pca':\n",
    "        pca = PCA(n_components=n_dim)\n",
    "        transformed = pca.fit_transform(data)\n",
    "        return transformed\n",
    "    #Not working should be same mds algorithm as matlab\n",
    "    elif method == 'mds':\n",
    "        mds = pcoa(data, number_of_dimensions=n_dim)\n",
    "        transformed = mds.samples\n",
    "        return transformed\n",
    "    elif method == 'tsne':\n",
    "        tsne = TSNE(n_components=n_dim)\n",
    "        transformed = tsne.fit_transform(data)\n",
    "        return transformed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cgr.py\n",
    "import numpy as np\n",
    "\n",
    "def cgr(chars, order, k):\n",
    "    \"\"\"computes CGR representation in standard format: C top-left, G top-right, A bottom-left, T bottom-right\n",
    "\n",
    "    Keyword arguments:\n",
    "    chars: sequence\n",
    "    order: chars to include in CGR\n",
    "    k: value of k-mer\n",
    "    \"\"\"\n",
    "    # set a numpy array of size 2^k,2^k  # remember that arrays are numbered top to bottom & left to right, unlike coordinate plots which go bottom up and left to right\n",
    "    out = np.zeros((2**k,2**k))\n",
    "    # set starting point of cgr plotting in the middle of cgr (x,y)\n",
    "    x = 2**(k-1) \n",
    "    y = 2**(k-1)\n",
    "\n",
    "    for i in range(len(chars)):\n",
    "        char = chars[i]\n",
    "        # devide x coordiate in half, moving it halfway to the left, this is correct if base is C or A\n",
    "        x = int(x/2)\n",
    "        # check to see if base is actually a G or T\n",
    "        if char == order[2] or char == order[3]:  # if the nucleotide is G or T\n",
    "            # add 2^(k-1) aka half the cgr length to the x value, brining it from 1/4 to 3/4\n",
    "            x += 2**(k-1)\n",
    "        # devide y coordiate in half, moving it halfway to the top, this is correct if base is C or G\n",
    "        y = int(y/2)\n",
    "        if char == order[0] or char == order[3]:  # if the nucleotide is A or T\n",
    "            # add 2^(k-1) aka half the cgr length to the y value, brining it from 1/4 to 3/4\n",
    "            y += 2**(k-1)\n",
    "        # if i+1 is greater than or equal to k (i.e. if the position of the base is greater than k )\n",
    "        if (i+1) >= k:\n",
    "            # add plus 1 to the positions y & x in the cgr array\n",
    "            out[y][x] += 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpers.py\n",
    "import math, sys\n",
    "import numpy as np\n",
    "from statistics import median, mean\n",
    "from mpl_toolkits import mplot3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def length_calc(seq_list):\n",
    "    \"\"\"calculates length stats\n",
    "\n",
    "    Keyword arguments:\n",
    "    seq_list: a list of squence\n",
    "    \"\"\"\n",
    "    len_list = map(len, seq_list)\n",
    "    max_len = max(len_list)\n",
    "    min_len = min(len_list)\n",
    "    mean_len = mean(len_list)\n",
    "    med_len = median(len_list)\n",
    "\n",
    "    return max_len, min_len, mean_len, med_len\n",
    "\n",
    "def inter_cluster_dist(clsuter,unique_clusters,distance_matrix, cluster_num):\n",
    "    avg_dist = np.zeros((cluster_num,cluster_num))\n",
    "    c_ind = np.zeros(cluster_num)\n",
    "    for h in range(cluster_num):\n",
    "        c_ind[h] = (clsuter == unique_clusters[h])\n",
    "    \n",
    "    for i in range(cluster_num):\n",
    "        for j in range(i+1, cluster_num):\n",
    "            if i==j:         \n",
    "                continue           \n",
    "            else:\n",
    "                dT = distance_matrix[c_ind[i],c_ind[j]]\n",
    "                avg_dist[i,j] = np.mean(np.transpose(dT), 1)  \n",
    "                avg_dist[j,i] = avg_dist[i,j]\n",
    "    return avg_dist\n",
    "\n",
    "# def mds(dMatPath):\n",
    "#     \"\"\"\n",
    "#     Takes input path to an nxn distance matrix. Performs Classical Multidimensional Scaling and returns an nx3 coordinate matrix, where each row \n",
    "#     corresponds to one of the input sequences in a 5-dimensional euclidean space. It also produces a 3D plot, very rough testing (need to color coat via cluster labels etc.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # to integrate with your code you can just change it to take input the distance matrix itself instead of the path.\n",
    "    \n",
    "#     dMat = np.loadtxt(dMatPath)\n",
    "\n",
    "#     eigValues, eigVectors = np.linalg.eig(dMat)\n",
    "#     idx = eigValues.argsort()[::-1][0:5]  \n",
    "#     selEigValues = eigValues[idx]\n",
    "#     selEigVectors = eigVectors[:,idx]\n",
    "\n",
    "#     if False in (selEigValues > 0):\n",
    "#         print(\"First 5 largest eigenvalues are not all positive. Exiting..\")\n",
    "#         sys.exit(-1)\n",
    "\n",
    "#     selEigVectors = np.array(selEigVectors)\n",
    "\n",
    "#     diagValues = []\n",
    "#     for i in range(len(selEigValues)):\n",
    "#         diagValues.append(math.sqrt(eigValues[i]))\n",
    "        \n",
    "#     diag = np.diag(diagValues)\n",
    "#     points = np.dot(selEigVectors,diag)\n",
    "\n",
    "#     minmaxScalingKameris = []\n",
    "#     for i in range(5):\n",
    "#         minmaxScalingKameris.append([ min(points[:,i]), max(points[:,i]) ])\n",
    "\n",
    "#     scaledPoints = []\n",
    "#     for i in range(len(dMat)):\n",
    "#         scaledPoints.append([0, 0, 0, 0, 0])\n",
    "#         for j in range(5):\n",
    "#             scaledPoints[i][j] = 2.0 *(points[i][j] - minmaxScalingKameris[j][0]) / ( minmaxScalingKameris[j][1] - minmaxScalingKameris[j][0]) - 1\n",
    "\n",
    "#     scaledPoints = np.array(scaledPoints) \n",
    "\n",
    "#     fig = plt.figure()\n",
    "#     ax = plt.axes(projection='3d')\n",
    "\n",
    "#     x = scaledPoints[:,0]\n",
    "#     y = scaledPoints[:,1]\n",
    "#     z = scaledPoints[:,2]\n",
    "\n",
    "#     ax.scatter(x, y, z, c='r', marker='o')\n",
    "\n",
    "#     fig.show()\n",
    "\n",
    "#     # return scaled data with first 5 dimensions\n",
    "\n",
    "#     return scaledPoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main script functions\n",
    "def compute_cgr(seq_index, seq_dict, keys, method_num, k_val, Result_path):\n",
    "    # seqs is the sequence database, it is being called by accession number\n",
    "    # (keys) which is being iterated over all seq_index,\n",
    "    # (count of all sequences in uploaded dataset) .seq calls the string\n",
    "    seq_new = str(seq_dict[keys[seq_index]].seq)\n",
    "    # Replace complementary Purine/Pyrimidine\n",
    "    if method_num == 15 or method_num == 16:\n",
    "        seq_new = seq_new.replace('G', 'A')\n",
    "        seq_new = seq_new.replace('C', 'T')\n",
    "    cgr_raw = cgr(seq_new, 'ACGT', k_val)  # shape:[2^k, 2^k]\n",
    "    # takes only the last (bottom) row but all columns of cgr to make 1DPuPyCGR\n",
    "    if method_num == 16:\n",
    "        cgr_output = cgr_raw[-1, :]\n",
    "    else:\n",
    "        cgr_output = cgr_raw\n",
    "    # shape:[2^k, 2^k] # may not be appropriate to take by column\n",
    "    fft_output = fft.fft(cgr_output, axis=0)\n",
    "    abs_fft_output = np.abs(fft_output.flatten())\n",
    "    np.save(os.path.join(Result_path,'Num_rep','cgr'+'k='+str(k_val)+'_'+str(seq_index)),cgr_output)\n",
    "    return abs_fft_output, fft_output, cgr_output  # flatted into 1d array\n",
    "\n",
    "def one_dimensional_num_mapping_wrapper(seq_index, method, seq_dict, keys, med_len):\n",
    "    # normalize sequences to median sequence length of cluster\n",
    "    seq_new = str(seq_dict[keys[seq_index]].seq)\n",
    "    if len(seq_new) >= med_len:\n",
    "        seq_new = seq_new[0:round(med_len)]\n",
    "    num_seq = method(seq_new) \n",
    "    if len(num_seq) < med_len:\n",
    "        pad_width = int(med_len - len(num_seq))\n",
    "        num_seq = pywt.pad(num_seq, pad_width, 'antisymmetric')[pad_width:]\n",
    "    fft_output = fft.fft(num_seq)\n",
    "    abs_fft_output = np.abs(fft_output.flatten())\n",
    "    return abs_fft_output, fft_output\n",
    "\n",
    "# def compute_pearson_coeffient(x, y, i, j):\n",
    "#     r = pearsonr(x, y)[0]\n",
    "#     normalized_r = (1-r)/2\n",
    "#     return normalized_r\n",
    "\n",
    "# def compute_pearson_coeffient_wrapper(abs_fft_output_list):\n",
    "#     distance_matrix = (1-np.corrcoef(abs_fft_output_list))/2\n",
    "#     return(distance_matrix)\n",
    "\n",
    "# def compute_pearson_coeffient_wrapper(i, j, abs_fft_output_list):\n",
    "#     # print(abs_fft_output_list)\n",
    "#     # x = abs_fft_output_list[i]\n",
    "#     # y = abs_fft_output_list[indices[1]]\n",
    "#     # print(x)\n",
    "#     # print(y)\n",
    "#     return compute_pearson_coeffient(abs_fft_output_list[i], abs_fft_output_list[j],i,j)\n",
    "\n",
    "def phylogenetic_tree(triangle_matrix):\n",
    "    names = keys\n",
    "    matrix = triangle_matrix\n",
    "    distance_matrix = DistanceMatrix(names, matrix)\n",
    "    constructor = DistanceTreeConstructor()\n",
    "    # neighbour joining tree not currently output\n",
    "    # nj_tree = constructor.nj(distance_matrix)\n",
    "    # neighbour_joining_tree = nj_tree.format('newick')\n",
    "    upgma = constructor.upgma(distance_matrix)\n",
    "    upgma_tree = upgma.format('newick')\n",
    "    print(upgma_tree, file=tree_print)\n",
    "    # can add code here for visualization with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User data setup\n",
    "# import cProfile, pstats\n",
    "# profiler = cProfile.Profile()\n",
    "# profiler.enable()\n",
    "## User set up, should be changed to argparse\n",
    "data_set='/Users/dolteanu/local_documents/Coding/MLDSP_dev_git/data/Primates/fastas'\n",
    "metadata = '/Users/dolteanu/local_documents/Coding/MLDSP_dev_git/data/Primates/metadata.csv'\n",
    "Run_name = 'Primates'\n",
    "# data_set = \"/Volumes/NVME-ssd/Gisaid data/Gisaid data 01:11:22/hcov_global_2022-01-09_23-30/Testing/Fastas\"\n",
    "# metadata = \"/Volumes/NVME-ssd/Gisaid data/Gisaid data 01:11:22/hcov_global_2022-01-09_23-30/Testing/test_metadata.csv\"\n",
    "# Run_name = \"Nextstrain>20\"\n",
    "Result_path = pathlib.Path(os.path.join('./Results',Run_name))\n",
    "if os.path.exists(Result_path):\n",
    "    shutil.rmtree(Result_path)\n",
    "    os.makedirs(os.path.join(Result_path,\"Num_rep\",'abs_fft'))\n",
    "else:\n",
    "    os.makedirs(os.path.join(Result_path,\"Num_rep\",'abs_fft'))\n",
    "# Dictionary order & names dependent for downstream execution \n",
    "methods_list = {1: num_mapping_PP, 2: num_mapping_Int, 3: num_mapping_IntN, 4: num_mapping_Real, 5: num_mapping_Doublet, 6: num_mapping_Codons, 7: num_mapping_Atomic,\n",
    "                8: num_mapping_EIIP, 9: num_mapping_AT_CG, 10: num_mapping_justA, 11: num_mapping_justC, 12: num_mapping_justG, 13: num_mapping_justT, 14: 'cgr',15: 'PuPyCGR', 16: '1DPuPyCGR'}\n",
    "# Change method number referring the variable above (between 1 and 16)\n",
    "method_num = 14\n",
    "k_val = 4  # used only for CGR-based representations(if methodNum=14,15,16)\n",
    "## End of user set up\n",
    "\n",
    "\n",
    "## Not currently implemented, for future development\n",
    "# test_set = None \n",
    "# seq_to_test = 0\n",
    "# min_seq_len = 0\n",
    "max_clust_size = 10000000\n",
    "# frags_per_seq = 1\n",
    "method = methods_list.get(method_num)\n",
    "seq_dict, total_seq, cluster_dict, cluster_stats = preprocessing(data_set,max_clust_size, metadata)\n",
    "print(cluster_stats)\n",
    "\n",
    "# Could be parallelized in the future\n",
    "\n",
    "# variable holding all the keys (accession numbers) for corresponding clusters\n",
    "keys = list(seq_dict.keys())\n",
    "# values = list(cluster_dict.values())\n",
    "#seq dict keys have to be in same order as cluster dict keys (see above)\n",
    "labels = [cluster_dict[x] for x in seq_dict.keys()]\n",
    "seqs_length = [len(seq_dict[keys[i]].seq) for i in range(total_seq)]\n",
    "med_len = median(seqs_length)\n",
    "print('Mean length'+ str(med_len))\n",
    "fft_output_list = []\n",
    "abs_fft_output_list = []\n",
    "cgr_output_list = []\n",
    "seq_new_list = []\n",
    "seq_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CGR or numerical representation & fourier transform \n",
    "# import cProfile, pstats\n",
    "# profiler = cProfile.Profile()\n",
    "# profiler.enable()\n",
    "print('Generating numerical sequences, applying DFT, computing magnitude spectra .... \\n')\n",
    "# for seq_index in range(total_seq):\n",
    "#     if method_num == 14 or method_num == 15 or method_num == 16:\n",
    "#         abs_fft_output, fft_output, cgr_output = compute_cgr(seq_index,seq_dict,keys,method_num,k_val)\n",
    "#         abs_fft_output_list.append(abs_fft_output)\n",
    "#         fft_output_list.append(fft_output)\n",
    "#         cgr_output_list.append(cgr_output)\n",
    "#         # seq_new_list.append(seq_new)\n",
    "#         # seq_list.append(seq)\n",
    "#     else:\n",
    "#         abs_fft_output, fft_output, seq_new, seq = one_dimensional_num_mapping_wrapper(seq_index)\n",
    "#         fft_output_list.append(fft_output)\n",
    "#         abs_fft_output_list.append(abs_fft_output)\n",
    "#         seq_new_list.append(seq_new)\n",
    "#         seq_list.append(seq)\n",
    "        \n",
    "pool = Pool()\n",
    "if method_num == 14 or method_num == 15 or method_num == 16:\n",
    "    for abs_fft_output, fft_output, cgr_output in pool.map(partial(compute_cgr, seq_dict=seq_dict, keys=keys, method_num=method_num, k_val=k_val), range(total_seq)):\n",
    "        abs_fft_output_list.append(abs_fft_output)\n",
    "        fft_output_list.append(fft_output)\n",
    "        cgr_output_list.append(cgr_output)\n",
    "else:\n",
    "    for abs_fft_output, fft_output in pool.map(partial(one_dimensional_num_mapping_wrapper, method = method, seq_dict=seq_dict, keys=keys, med_len=med_len), range(total_seq)):\n",
    "        abs_fft_output_list.append(abs_fft_output)\n",
    "        fft_output_list.append(fft_output)\n",
    "pool.close()\n",
    "# profiler.disable()\n",
    "#     # with open('profile2.txt','w') as p:\n",
    "# stats = pstats.Stats(profiler)\n",
    "# stats.strip_dirs()\n",
    "# stats.sort_stats('cumtime')\n",
    "# # stats.dump_stats('./profile2.prof')\n",
    "# stats.print_stats(30)\n",
    "# stats.print_callers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(cgr_output_list[0],cmap=cm.gray_r)\n",
    "plt.xticks([])\n",
    "plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance matrix\n",
    "print('Building distance matrix')\n",
    "\n",
    "# # hdf5 implementation\n",
    "# with h5py.File(\"distmat\"+Run_name+\".hdf5\", \"w\") as dist_mat_file:\n",
    "#     distance_matrix = dist_mat_file.create_dataset(\"dist_mat\", (total_seq, total_seq))\n",
    "#     for i in range(total_seq):\n",
    "#         for j in range(total_seq):\n",
    "#             distance_matrix[i,j] = compute_pearson_coeffient_wrapper(i,j,abs_fft_output_list)\n",
    "# reading file if already made\n",
    "# # rand = h5py.File('/Users/dolteanu/local_documents/Coding/MLDSP_dev_git/distmat.hdf5', 'r')\n",
    "# # distance_matrix = rand['dist_mat']\n",
    "\n",
    "# #parallel implementation\n",
    "# for normalized_r, i, j in pool.starmap(partial(compute_pearson_coeffient_wrapper, abs_fft_output_list=abs_fft_output_list), ((i, j) for i in range(total_seq) for j in range(total_seq)),chunksize=total_seq):\n",
    "#     distance_matrix[i,j] = normalized_r\n",
    "# distance_matrix = compute_pearson_coeffient_wrapper(abs_fft_output_list)\n",
    "# # Numpy implementation\n",
    "distance_matrix = (1-np.corrcoef(abs_fft_output_list))/2\n",
    "np.save(os.path.join(Result_path,'dist_mat'), distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Molecular distance map\n",
    "print('Scaling & data visualisation')\n",
    "le = pe.LabelEncoder()\n",
    "le.fit(labels)\n",
    "labs = le.transform(labels)\n",
    "scaled_distance_matrix = dimReduction(distance_matrix, n_dim=3, method='pca')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "ax.scatter(scaled_distance_matrix[:, 0], scaled_distance_matrix[:, 1], scaled_distance_matrix[:, 2],c=labs)\n",
    "plt.savefig(os.path.join(Result_path,'MoDmap.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML classification\n",
    "print('Performing classification .... \\n')\n",
    "folds = 10\n",
    "if (total_seq < folds):\n",
    "    folds = total_seq\n",
    "mean_accuracy, accuracy, cmatrix, misClassifiedDict = classify_dismat(distance_matrix, labels, folds, total_seq)\n",
    "# accuracy,avg_accuracy, clNames, cMat\n",
    "# accuracies = [accuracy, avg_accuracy];\n",
    "print('Classification accuracy 5 classifiers\\n', accuracy)\n",
    "print('**** Processing completed ****\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7472a9ec7f3fc71dcf9c782edfdc6069c62af0ae0e799e3cfe92580f5119c984"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
